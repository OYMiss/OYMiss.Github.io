<!DOCTYPE html><html><head><meta charset="utf-8"><title>奇异值分解与主成分分析</title>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link rel="stylesheet" href="/style/style.css"></head><body>
<header class="header" id="header">
    <div class="header-wrapper">
        <div class="logo">
            <h1><a href="/">OY's Home</a></h1>
        </div>
        <nav class="main-nav">
            <ul class="menu">
                <li class="menu-item">
                    <a href="/" id="home">
                        <span class="base-name">
                            Home
                        </span>
                    </a>
                </li>
                <li class="menu-item">
                    <a href="/archives" id="archives">
                        <span class="base-name">
                            Archives
                        </span>
                    </a>
                </li>
                <li class="menu-item">
                    <a href="/categories" id="categories">
                        <span class="base-name">
                            Categories
                        </span>
                    </a>
                </li>
                <li class="menu-item">
                    <a href="/about" id="about">
                        <span class="base-name">
                            About
                        </span>
                    </a>
                </li>
            </ul>
        </nav>
    </div>
</header>
<article class="markdown-body"><h1>奇异值分解与主成分分析</h1><h3>奇异值分解（SVD）</h3><p>如果矩阵 \(A_{m\times n}\) 不能够进行特征值分解，要考虑使用<strong>奇异值分解（singular value decomposition，SVD）</strong> 化成：</p>\[A = U\Sigma V^T
\]<p>其中，\(U_{m\times m}\) 和 \(V_{n\times n}\) 都是正交矩阵，\(\Sigma_{m\times n}\) 是一个矩形对角矩阵 （rectangular diagonal matrices），并且 \(\Sigma\) 主对角线上的元素都是非负的，记作 \(\sigma_i\)。</p>\[AA^T = U\Sigma V^T V\Sigma^T U^T = U \Sigma\Sigma^T U^T
\]<p>其中 \(AA^T\) 是对称矩阵，一定能够对角化。</p><p><!--more--></p><p>\(U\) 被称作 \(A\) 的<strong>左奇异向量 （left-singular vectors）</strong>，它是 \(AA^T\) 的特征向量的一个集合。</p><p>\(V\) 被称作 \(A\) 的<strong>右奇异向量 （right-singular vectors）</strong>，它是 \(A^TA\) 的特征向量的一个集合。</p><p>\(AA^T\) 与 \(A^TA\) 拥有同样的特征值，于是 \(\sigma_i^2\) 是 \(AA^T\) 的特征值。</p><p>相对于特征值分解中 \(Sv_i = \lambda v_i\)，奇异值分解有：</p>\[Av_i = \sigma_i u_i \qquad i = 1, \ldots, \min(m, n)
\]<p>可以把矩阵 \(A\) 看作是一个变换，将 \(\mathbb{R}^m\) 变换到 \(\mathbb{R}^n\)，其中 \(\mathbb{R}^m\) 的基向量为 \(u_i\)，\(\mathbb{R}^n\) 的基向量为 \(v_i\)。</p><h3>主成分分析（PCA）</h3><p><strong>主成分分析（Principal components analysis，PCA）</strong> 是一种统计分析、简化数据集的方法。它利用正交变换来对一系列可能相关的变量的观测值进行线性变换，从而投影为一系列线性不相关变量的值，这些不相关变量称为主成分（Principal Components）。</p><p>用矩阵表示就是</p>\[T = XW
\]<p>把数据变换到新的坐标系统后，使得第一主成分描述的是最大方差的方向，第二主成分是第二大方差的方向，以此类推。</p><p>对于每个主成分有：\(t_{k(i)} = x_{i} \cdot w_{(k)}\)。</p><p>为了使得方差最大化，第一个权重向量 \(w_1\) 应该满足：</p>\[\begin{align*} \\
 w_{(1)}
 & = \underset{\Vert \mathbf{w} \Vert = 1}{\arg \max} \{ \sum(t_1)^2_{(i)}\} \\
 & = \underset{\Vert \mathbf{w} \Vert = 1}{\arg \max} \{ \sum(x_{(i)}\cdot w)^2\} \\
 & = \underset{\Vert \mathbf{w} \Vert = 1}{\arg \max} \{{\Vert Xw \Vert}^2\} \\
 & = \underset{\Vert \mathbf{w} \Vert = 1}{\arg \max} \{ w^TX^TXw \} \\\\
\end{align*}
\]<p>当 \(w_{(1)}\) 为单位向量时，</p>\[w_{(1)} = \arg \max \left\{ \frac{w^TX^TXw}{w^Tw} \right\}
\]<p><strong>瑞利商（Rayleigh quotient）</strong>的定义是：</p>\[R(M, x) = \frac{x^*Mx} {x^*x}
\]<p>如果 \(M\) 矩阵是一个 Hermitian 矩阵，实数范围内就是对称矩阵，那么有 \(R(M, x) \in [\lambda_{min}, \lambda_{max}]\)，如果取最大值，则 \(x\) 为最大的特征值对应的一个特征向量。</p><p>所以 \(w_{(1)}\) 的就是 \(X^TX\) 的最大特征值所对应的特征向量，也就是 \(X\) 的最大奇异值在 \(V\) 中所对应的右奇异向量。</p><p>从 \(X\) 中减去前 \(k - 1\) 个主成分得到 \(\hat{X_k}\)。</p>\[\hat{X_k} = X - \sum_{s = 1}^{k - 1}Xw_{(s)}w_{(s)}^T
\]\[w_{(k)} = \arg \max \left\{ \frac{w^T\hat{X_k}^T\hat{X_k}w}{w^Tw} \right\}
\]<p>经过计算可以得出 \(\hat{X_k}^T\hat{X_k}\) 与 \(X^TX\) 的特征值及特征向量相同，可以得到 \(w_{(k)}\) 是 \(X^TX\) 第 \(k\) 大的特征值对应的特征向量。</p><p>最后可以写成：</p>\[T = XW
\]<p>其中 \(W_{p \times p}\) 是权重矩阵，它的列向量是 \(X^TX\) 的特征向量，\(X\) 是一个 \(n \times p\) 的矩阵，\(n\) 可以代表实验次数，\(p\) 可以代表特征的个数。</p><p>可以只保留前 \(L\) 主成分，得到 \(T_L = X W_L\)，其中 \(T_L\) 是一个 \(n \times L\) 的矩阵，\(W_L\) 是一个 \(p \times L\) 的矩阵。</p><h3>将 SVD 用于 PCA</h3><p>将 \(X\) 做 SVD 得到 \(X = U\Sigma W^T\)，于是 \(T\) 可以写成：</p>\[\begin{align*}
T
&= XW \\
&= U\Sigma W^TW \\
&= U\Sigma
\end{align*}
\]<p>\(T\) 矩阵可以变成左奇异向量乘以对应的奇异值得到，这个叫做<strong>极分解（Polar decomposition）</strong>。</p><p>对于矩阵 \(A\)，如果取出 \(k\) 个最大的奇异值，得到 \(A_k\)。</p>\[A = U \Sigma V^T = \sigma_1u_1v_1^T + \dots + \sigma_ru_rv_r^T \\
A_k = U_k \Sigma_k V_k^T = \sigma_1u_1v_1^T + \dots + \sigma_ku_kv_k^T
\]<p><strong>Eckart–Young 定理</strong>：如果 \(B\) 矩阵的秩为 \(k\)，那么 \(\Vert A - B \Vert \ge \Vert A - A_k \Vert\)。</p><p>被截取的 \(n\times L\) 矩阵 \(T_L\) 可以通过获得 \(L\) 个最大的奇异值及其对应的奇异向量组成。</p>\[T_L = U_L\Sigma_L = X W_L
\]<p>根据 Eckart–Young 定理，通过这种截断的奇异值分解得到的矩阵，\(T_L\) 或者 \(A_L\) 是最接近的原来矩阵的秩为 \(L\) 的矩阵。</p><p>[1] <a href="https://ocw.mit.edu/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/">Matrix Methods in Data Analysis, Signal Processing, and Machine Learning</a></p><p>[2] <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principal component analysis</a></p><p>[3] <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition#Geometric_meaning">Singular value decomposition</a></p><p>[4] <a href="https://seanwangjs.github.io/2017/11/27/rayleigh-quotient-maximum.html">瑞利商与极值计算</a></p><p>[5] <a href="https://en.wikipedia.org/wiki/Rayleigh_quotient">Rayleigh quotient</a></p></article></body></html>